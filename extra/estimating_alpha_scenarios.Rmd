---
title: "Exploring Options for Alpha Estimation"
author: "Susan Vanderplas"
date: "8/21/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

Main factors in estimating $\alpha$:

- One-target lineup vs. Rorschach lineup
  - Data plot: does it count as a null plot?
  - Which distribution to compare to? $m=19$ or $m=20$, $K=K$ or $K=(K-\#$ data plot selections$)$?
- Can we estimate from a single lineup, or do we need many?
- Type of estimation: visual, numerical optimization (quasi-method-of-moments), or ML?


***

In practice, we can envision several different scenarios:

1. Run a pilot study of several Rorschach lineups    
In this scenario, we would use $m=20$ and ensure exactly $K$ evaluations. There are a couple of options once the pilot data is collected:

    a. Take the average number of panels with $c_i > c$ selections, then optimize numerically using the expected number of panels with $c_i > c$ selections to determine $\hat\alpha$. 
    b. Take the most typical range of the number of null plots with $c_i > c$ selections. Use visual estimation method to select $\hat\alpha$
        - at the lower end of the range (overly conservative)
        - in the middle of the range
        - at the upper end of the range (walking the line between well calibrated and liberal, depending on worst-case or best-case scenario)
    c. Use the (unstable) ML estimator for the Multinomial-Dirichlet

2. Use a **set** of one-target lineups as if they were Rorschach lineups (e.g. operate under $H_0$: there is no difference between the null and data plot)    
Under this scenario, the worst-case possibility (for $\hat\alpha$) is that the data plots are radically different than the null plots, which would lead to under-estimation of $\alpha$ and a higher p-value; when the $\hat\alpha$ is later used for p-value calculation, the p-value would be nonsignificant (in the pathological case). More commonly, though, this would just lead to an overly conservative estimate so long as there is significant mass on the null plots. The best-case possibility (for $\hat\alpha$) is that the data plots are not radically different than the null plots, so the number of plots identified is accurate and the value of $\hat\alpha$ is accurate.     
*This scenario is basically useless if there are a lot of selections of the data plot, but may be more useful in e.g. the scenario like the example in the paper, where the number of data plot selections isn't* that *overwhelming*. 

3. Use a **set** of one-target lineups, excluding the data plot from consideration and working with the remaining nulls. Here, $K$ cannot be controlled (because participants would have selected from $m=20$ plots, one of which is a data plot, and $K_{null} < K$). 

    a. Separately estimate each $\hat\alpha_j$ for lineups $j=1, ...$ using numerical optimization and pool the estimates somehow (mean? median? mode?), using $m=19$ for the estimation process
    b. Separately estimate each $\hat\alpha_j$ for lineups $j=1, ...$ using numerical optimization and pool the estimates somehow (mean? median? mode?), using $m=20$ for the estimation process. This will slightly under-estimate the number of null plot selections (x vs 20/19*x), but may make for easier comparisons/generalizations.
    c. Group plots into rough categories based on total number of null plot identifications. Generate several separate curves, and use the visual estimation method on each set of plots with roughly similar number of null plot identifications. Pool the $\hat\alpha$s to get a consensus estimate, using $m=19$ for the estimation process.
    d. Group plots into rough categories based on total number of null plot identifications. Generate several separate curves, and use the visual estimation method on each set of plots with roughly similar number of null plot identifications. Pool the $\hat\alpha$s to get a consensus estimate, using $m=20$ for the estimation process. This will slightly under-estimate the number of null plot selections (x vs 20/19*x), but may make for easier comparisons/generalizations.
    
4. Re-evaluate a set of one-target lineups, with the data plot blanked out. This is much the same as (3) except that $K$ can be controlled, so the expected number of panels selected can be computed based on the lineup evaluations, pooled, and only one $\hat\alpha$ estimate needs to be generated (visually or numerically). Best option would be to compare against $m=19$. 

***

Define $\bf c = (\tilde{\bf c}, c_t)'$, where $c_t$ is the number of target plot selections (the panels are exchangeable) and $\bf\alpha = (\tilde{\bf\alpha},\alpha_t)$ where $\tilde{\bf\alpha} = \alpha \cdot 1_{1\times(m-1)}$ and $\alpha_t = \alpha \cdot 1_{1\times1}$. 

Let $K = \sum_{i=1}^m c_i$ be the total number of evaluations, and $K - c_t$ be the total number of null plot evaluations. 
\begin{align}
f(\bf c_{1\times m}|\bf\alpha_{1\times m}) & = \frac{\left(\sum_{i=1}^m c_i\right)!}{\prod_{i=1}^m c_i!} \frac{B(\alpha + \bf c)}{B(\bf\alpha)}\\
& = \left(\frac{K!}{\prod_{i=1}^m c_i!}\right)\left(\frac{B(\bf\alpha + \bf c)}{B(\bf\alpha)}\right)
\end{align}

Now, 
$$\frac{K!}{\prod_{i=1}^m c_i!} = \frac{K!}{c_t!\cdot\prod_{i=1}^{m-1} c_i!}\cdot\frac{(K-c_t)!}{(K-c_t)!} = \frac{K!}{(K-c_t)!c_t!} \frac{(K-c_t)!}{\prod_{i=1}^{m-1}c_i!} = \binom{K}{c_t}\frac{(K-c_t)!}{\prod_{i=1}^{m-1}c_i!}$$

Also, 
\begin{align}
\frac{B(\bf\alpha + \bf c)}{B(\bf\alpha)} &= \frac{\prod_{i=1}^m\Gamma(\alpha + c_i)}{\Gamma(m\alpha + K)}\cdot\frac{\Gamma(m\alpha)}{\prod_{i=1}^m\Gamma(\alpha)}\\
&=\frac{\prod_{i=1}^m\Gamma(\alpha + c_i)}{\Gamma(m\alpha + K)}\cdot\frac{\Gamma(m\alpha)}{\prod_{i=1}^m\Gamma(\alpha)}\cdot\frac{\Gamma\left((m-1)\alpha + K - c_t\right)}{\Gamma\left((m-1)\alpha + K - c_t\right)}\cdot\frac{\Gamma\left((m-1)\alpha \right)}{\Gamma\left((m-1)\alpha\right)}\\
& = \frac{\Gamma(\alpha + c_t)\Gamma\left((m-1)\alpha + K - c_t\right)}{\Gamma(m\alpha + K)}\cdot 
\frac{\Gamma(m\alpha)}{\Gamma(\alpha)\Gamma\left((m-1)\alpha\right)}\cdot
\frac{\prod_{i=1}^{m-1}\Gamma(\alpha + c_i)}{\Gamma\left((m-1)\alpha + K - c_t\right)}\cdot
\frac{\Gamma\left((m-1)\alpha\right)}{\prod_{i=1}^{m-1}\Gamma(\alpha)}\\
& = \frac{B(\alpha + c_t, (m-1)\alpha + K - c_t)}{B(\alpha, (m-1)\alpha)}\frac{B(\tilde\alpha + \tilde{\bf c})}{B(\tilde\alpha)}
\end{align}

So 
\begin{align}
f(\bf c_{1\times m}|\bf\alpha_{1\times m}) & = \left(\frac{K!}{\prod_{i=1}^m c_i!}\right)\left(\frac{B(\bf\alpha + \bf c)}{B(\bf\alpha)}\right)\\
& = \binom{K}{c_t}\frac{(K-c_t)!}{\prod_{i=1}^{m-1}c_i!}\cdot \frac{B(\alpha + c_t, (m-1)\alpha + K - c_t)}{B(\alpha, (m-1)\alpha)}\frac{B(\tilde\alpha + \tilde{\bf c})}{B(\tilde\alpha)}\\
& = \binom{K}{c_t}\frac{B(\alpha + c_t, (m-1)\alpha + K - c_t)}{B(\alpha, (m-1)\alpha)}\cdot\frac{(K-c_t)!}{\prod_{i=1}^{m-1}c_i!}\frac{B(\tilde\alpha + \tilde{\bf c})}{B(\tilde\alpha)}\\
& = f(c_t | K, \alpha, (m-1)\alpha) \cdot f(\bf c_{1\times(m-1)} | \alpha_{1\times(m-1)})
\end{align}

That is, conditional on the overall total number of evaluations, the null plot evaluations have a Multinomial-Dirichlet distribution with hyperparameter $\alpha$, independent of the target plot selections. 

So we can estimate $\alpha$ from the null plot selections, if there are enough null plot selections.

***

How many null plot selections are necessary?

```{r, eval = F}
library(tidyverse)
library(gtools) # rdirichlet

n_null_sels <- function(K, theta, threshold) {
  sum(rmultinom(1, size = K, prob = theta) > threshold)
}

expected_panels <- function(alpha, ev, m, K, threshold) {
  x <- (threshold + 1):K
  (ev - m/beta(alpha, (m - 1)*alpha)*sum(choose(K, x)*beta(x + alpha, K - x + (m - 1)*alpha)))^2
}

est_alpha <- function(params) {
  # browser()
  N <- 100
  m <- 20
  threshold <- 1
  stopifnot("alpha" %in% names(params))
  stopifnot("K" %in% names(params))
  list2env(params, envir = as.environment(-1))
  
  thetas <- rdirichlet(N, rep(alpha, m))
  counts <- rmultinom(1, size = K, prob = thetas) %>%
    matrix(nrow = m, ncol = N)
  ev <- mean(colSums(counts > threshold))
  as <- seq(0.01, 100, .001)
  ests <- purrr::map_dbl(as, ~expected_panels(., ev, m, K, threshold))
  # ests <- optimize(expected_panels, c(0.01, 100), ev = ev, threshold = threshold, m = m, K = K)
  
  tibble(alpha_est = as[which.min(ests)], optim_obj = min(ests),
         alpha_actual = alpha, ev = ev, sim = list(data.frame(counts = counts, thetas = t(thetas))))
}

try_est_alpha <- safely(est_alpha)

sim_params <- crossing(K = seq(5, 30, by = 5), alpha = c(0.05, 0.1, 0.25, 0.5, 1), N = 1:10, idx = 1:10) %>%
  mutate(row = row_number()) %>%
  mutate(m = 19, threshold = 1) %>%
  nest(data = -c(idx, row)) %>%
  mutate(res = purrr::map(data, try_est_alpha))

sim_params2 <- sim_params %>% 
  mutate(res = purrr::map(res, "result")) %>% 
  unnest(res) %>% 
  unnest(data)
```

Simulation:

- Between 5 and 50 evaluations of 
- Between 1 and 20 null plots
- repeated 10x

```{r, eval = F}
filter(sim_params2, K ==5,
       # optim_obj < 0.05
       ) %>%
  ggplot(aes(x = alpha, y = alpha_est)) + 
  facet_wrap(~N) +
  geom_jitter() + 
  scale_x_log10() + scale_y_log10()

```