\documentclass[APA,LATO1COL]{WileyNJD-v2}
% \usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{mathrsfs}
\usepackage{bm}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage{endfloat} % Figures to the end of the document

\usepackage{cleveref}

\usepackage{anyfontsize}% remove font warnings
\usepackage{subcaption}  % an alternative package for sub figures
%\renewcommand{\subfloat}[2][need a sub-caption]{\subcaptionbox{\normalsize #1}{#2}}
% --- Editing ------------------------------------------------------------------

\newcommand{\hh}[1]{{\textcolor{orange}{#1}}}
\newcommand{\svp}[1]{{\textcolor{magenta}{#1}}}
\newcommand{\dc}[1]{{\textcolor{teal}{#1}}}

% ------------------------------------------------------------------------------

\graphicspath{{figure/}}

\articletype{Article Type}%

\received{26 April 2016}
\revised{6 June 2016}
\accepted{6 June 2016}

\raggedbottom

\setlength\parindent{0pt} % noindent for the whole document

\begin{document}
% \tableofcontents
\title{Statistical Significance Calculations for Scenarios in Visual Inference}
%\protect\thanks{This is an example for title footnote.}}

\author[1]{Susan Vanderplas*}

\author[2]{Christian R\"ottger}

\author[3]{Dianne Cook}

\author[4]{Heike Hofmann}

\authormark{Vanderplas \textsc{et al}}


\address[1]{\orgdiv{Department of Statistics}, \orgname{University of Nebraska-Lincoln}, \orgaddress{\state{Nebraska}, \country{United States}}}

\address[2]{\orgdiv{Department of Mathematics}, \orgname{Iowa State University}, \orgaddress{\state{Iowa}, \country{United States}}}

\address[3]{\orgdiv{Department of Econometrics and Business Statistics}, \orgname{Monash University}, \orgaddress{\state{Victoria}, \country{Australia}}}

\address[4]{\orgdiv{Department of Statistics}, \orgname{Iowa State University}, \orgaddress{\state{Iowa}, \country{United States}}}

\corres{*Susan Vanderplas, \email{susan.vanderplas@unl.edu}}

% \presentaddress{This is sample for present address text this is sample for present address text}


\abstract[Summary]{Statistical inference provides the protocols for conducting rigorous science, but data plots provide the opportunity to discover the unexpected. These disparate endeavors are bridged by visual inference, where a lineup protocol can be employed for statistical testing. Human observers are needed  to assess the lineups, typically using a crowd-sourcing service.  This paper describes a new approach for computing statistical significance associated with the results from applying a lineup protocol. It utilizes a Dirichlet distribution to accommodate different levels of visual interest in individual null panels. The suggested procedures facilitate statistical inference for a broader range of data problems.}

\keywords{data visualization, statistical graphics, hypothesis testing, data science}

\maketitle

%\footnotetext{\textbf{Abbreviations:} ML, maximum likelihood;}


<<setup, echo = F, include = F>>=
library(tidyverse)
library(mgcv)
library(Cairo)
knitr::opts_chunk$set(echo = F, message = F, warning = F, dev = c("cairo_pdf", "cairo_ps"), dev.args = list(fallback_resolution = 800), dpi = 800)
theme_set(theme_bw())
@

<<code, include = F>>=
source("code/functions.R")
@

\section{Introduction}

% ---- Gentle introduction/ power of graphics ----------------------------------
% ---- Graphics and hypothesis testing -----------------------------------------
Graphics provide the opportunity to understand statistical data at a (visual) sensory level. They are also important for data analysis because a plot can communicate more than summary statistics~\citep{anscombe:1972,matejka:2017}.
Statistical graphics allow us to leverage the full bandwidth of the \hh{human} visual system for implicit data analysis. Because this analysis is implicit, we often assume these visual analyses are not decisive in the same way that a hypothesis test is decisive.
Graphics generally do not come with a significance threshold, and in many cases, we do not explicitly declare the hypothesis we might be testing before viewing the data plot.
\citet{buja:2009} introduced a protocol for \emph{visual inference} that allows for formal hypothesis testing using graphical displays and visual evaluation.

To test whether a chart shows a visually significant result, we can use the same machinery used by randomization tests: 
(1) construct a method to generate data consistent with the null hypothesis, such as using randomization or drawing samples from a model consistent with the null hypothesis, (2) generate many copies of the test statistic (in this case, the plot), and (3) see where the observed statistic falls in the distribution of artificially generated quantities.
The conceptual framework was described in \citet{buja:2009}, who introduced two protocols: a lineup, and a Rorschach (or null) lineup.

An assembly of several null plots with a target (or data) plot is called a \emph{lineup}, named after the law-enforcement procedure to line up a  suspect among a set of innocents  to check if a victim can identify the suspect as the perpetrator of the crime. 
In its visual version, lineups are typically composed of $m-1$ ``null" plots (generated under the null hypothesis) and one data plot containing the observed data. 

The Rorschach lineup is named after the ink-blot test \citep{exner2003rorschach} historically used in psychoanalysis; as in the inkblot test, the Rorschach lineup provides ambiguous visual signals which are open to interpretation.
Figure \ref{fig:typesoflineups} shows several examples of each type of lineup.
In a Rorschach lineup, all plots are null plots; the purpose is to assess the extent of visual variation that occurs in data generated by the null mechanism.


% ---- Graphs as statistics ----------------------------------------------------
The fundamental premise of visual inference is that charts are visual statistics: summaries of data sets generated by mathematical functions.
Underlying this premise is the concept of a grammar of graphics initially laid out by \citet{wilkinson:1999}, and further developed in \citet{hadley:2009}, which provides a functional mapping from variables (abstractly defined) into graphical elements of a plot.
This abstract plot definition allows us to declare a hypothesis by specifying the relationship between the variables and the spatial elements of the plot.
% to come from under the rug (implicit), be explicitly declared.
For example, the scatter plots shown in the one target lineups in Figure \ref{fig:typesoflineups}b map variable 1 to the $x$-axis and variable 2 to the $y$-axis, with the intent to study the association between the two variables.

% ---- Lineups as tests --------------------------------------------------------
\hh{The natural next step} is to consider using these visual statistics to conduct hypothesis tests.
In the scatterplot example above, the null hypothesis is that there is no association between the two variables and the corresponding alternative hypothesis is that there is an association.
Conducting a test using the observed data plot requires that we compare the data plot to a reference distribution.
Any null generating mechanism, such as randomization or sampling from a known distribution, may be used to generate the data for the null panels in the lineup.

In a numerical statistical test, the summary statistics are naturally ordered, and the test statistic is compared to quantities consistent with the null hypothesis; often, a $p$-value is used to assess the probability that the observed statistic would arise by chance under the null hypothesis.
When conducting visual inference, the statistics have no such natural ordering; instead, our statistics must be evaluated by human participants.
Typically, graphical tests utilize a service like Amazon Mechanical Turk~\citep{turk} to acquire evaluations of lineups.
An extreme visual test statistic is one that is easily distinguishable from other null plots in the lineup; that is, if the data panel is identified when the lineup is evaluated, we would reject the null hypothesis.
Thus, lineups have the features of a \emph{statistical test}.
One notable difference between visual and numerical statistical tests is that visual tests are more comprehensive: individuals are asked to select one or more plots from the lineup which are ``different", but typically, the specific type of difference is left unspecified.
As a result, a visual test might evaluate several simultaneous characteristics of a plot, where the equivalent numerical assessment may involve multiple tests using different test statistics.
Occasionally, because there is some inductive reasoning required of the participant, the feature used to select the ``different" panel is not the feature under investigation by the experimenter.

Typically, lineups consist of $m = 20$ panels, $m_0 = 19$ of which are generated from the null distribution.
Under this formulation, a single evaluation of a lineup in which the data panel is selected would have a visual $p$-value of $p\leq 0.05$.
In most visual inference experiments, each lineup is evaluated by multiple individuals, and the aggregated results are used to generate a visual $p$-value using a Binomial model as described in \citet{majumder2013validation}.
This approach is simple,  but does not account for dependencies in the design of most visual inference experiments resulting from repeated evaluations of the same lineup.

In this paper, we propose a new approach for computing $p$-values using a Dirichlet-multinomial distribution to model the probabilities of selecting each panel and the observed participant selections.
This model is more flexible than the binomial model used previously, and better represents the perceptual and statistical dependencies present when lineups are evaluated by multiple observers.
Leveraging this model, we propose a method for estimation of the model parameters and present a diagnostic procedure for null plot generation models.

<<typesoflineups, eval = T,echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=6, fig.cap = "Types of lineups. In a Rorschach, all plots are null plots, with a purpose of understanding patterns that may occur by chance. A one target lineup has one data plot, and the remaining are null plots.">>=
library(nullabor)
library(gridExtra)
set.seed(2020)
df <- tibble(x=runif(100)) %>%
  mutate(y1 = -4*(x-0.5) + rnorm(100)) %>%
  mutate(y2 = ifelse(x<0.5, (x-0.5) + 5 + rnorm(100),
                     (x-0.5) - 5 + rnorm(100)))
p1 <- ggplot(rorschach(null_permute('y1'), df, n=5), aes(x, y1)) +
  geom_point() +
  facet_wrap(~ .sample, ncol=5) +
  theme(aspect.ratio=1,
        axis.ticks = element_blank(),
        axis.text = element_blank()) +
  xlab("") + ylab("") +
  ggtitle("a. Rorschach")
p2 <- ggplot(lineup(null_permute('y1'), df, n=5), aes(x, y1)) +
  geom_point() +
  facet_wrap(~ .sample, ncol=5) +
  theme(aspect.ratio=1,
        axis.ticks = element_blank(),
        axis.text = element_blank()) +
  xlab("") + ylab("") +
  ggtitle("b. One target lineup")
grid.arrange(p1, p2, ncol=1)
@

\subsection{Modeling Lineup Panel Selections}\label{sec:alpha}

The lineup evaluation task boils down to a selection of one of $m$ panels in a lineup. 
We model the probability of selecting panel $i$ as $\theta_i, i = 1, ..., m$, where $\bm\theta \sim Dirichlet(\bm\alpha)$. 
The $m$-dimensional Dirichlet distribution generates $\bm\theta$ distributed on the $m-1$ simplex, that is, $\sum \theta_i = 1$. 
When $\alpha_1 = \alpha_2 = ... = \alpha_m$, the distribution is a symmetric Dirichlet distribution.  As all null plots are generated using the same process, we model the picking probability $\bm\theta$ for a set of null plots using a symmetric Dirichlet distribution.

% Stolen from heike/lineup-scenarios
When $\alpha = 1$, the symmetric Dirichlet distribution is uniform on the ($m-1$)-dimensional simplex.
When $\alpha < 1$, the mass of the distribution is along the edges of the simplex, where most values of $\theta_i$ will be close to 0. When $\alpha>1$, the mass of the distribution is in the center of the simplex, with most of the $\theta_i$ having similar values.
\Cref{fig:simplex} shows ternary plots~\citep{ggtern} of values simulated from a 3-dimensional Dirichlet distribution which illustrate the effect of $\alpha$ on the sampled $\bm\theta$.

<<simplex, eval = F,echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=4, fig.cap = "Dirichlet distributed samples on the 2-dimensional simplex.">>=
simplex <- purrr::map_df(.x = c(1/3, 1, 3), ~data.frame(
  gtools::rdirichlet(5000, alpha = rep(.x, 3))
), .id = "alpha") %>%
  rename(p1 = X1, p2 = X2, p3 = X3) %>%
  mutate(alphalabel = factor(alpha, levels = 1:3, labels = c("alpha: 1/3", "alpha: 1", "alpha: 3")))

p <- ggtern::ggtern(aes(x = p1, y = p2, z = p3), data = simplex) +
  geom_point(alpha = .075, shape = 16) +
  facet_wrap(~alphalabel, labeller = "label_parsed") +
  theme(panel.background = element_rect(colour = "black"))
print(p)
@

\begin{figure}
\includegraphics[width=\textwidth]{figure/simplex-1.pdf}
\caption{Dirichlet distributed samples on the 2-dimensional simplex. $\alpha = 1$ is a uniform distribution on the simplex, $\alpha < 1$ up-weighs the (hyper)faces of the simplex, $\alpha > 1$ puts more weight towards the center of the simplex.\label{fig:simplex}}
\end{figure}

%Marginal Beta($\alpha, 2\alpha$) densities corresponding to \\Cref{fig:simplex}.
<<betas, echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=4, fig.cap="Marginal Beta($\\alpha, 2\\alpha$) densities corresponding to the above Dirichlet densities.">>=
p <- seq(0,1, by=0.005)
alpha <- c(1/3,1,3)
betas <- data.frame(expand.grid(p=p, alpha=alpha))
betas$density <- with(betas, dbeta(p, alpha, 2*alpha))
betas$density[is.infinite(betas$density)] <- 0
betas <- rbind(betas, data.frame(p=0,alpha=1,density=0))
betas$alphalabel <- factor(betas$alpha)
levels(betas$alphalabel) <- expression("alpha: 1/3", "alpha: 1", "alpha: 3")
ggplot(data=betas, aes(x=p, y=density)) +
  geom_polygon(fill=I("grey50"), alpha=I(.8), colour=I("grey20")) +
  ylab("Density") + facet_wrap(~alphalabel, labeller="label_parsed") +
  ggplot2::theme_bw()
@

% End stolen from heike/lineup-scenarios

While graphical illustrations of the $m$-dimensional Dirichlet distribution are more difficult when $m>3$, we can use simulation to assess the meaning of $\alpha$ as it relates to lineup panel selection probabilities $\theta_i$. \Cref{fig:prior-predictive} shows simulated selection probabilities $\theta_i$, sorted such that the panel with the highest selection probability is first,  for several values of $\alpha$ in a $m=20$ panel lineup.
It is evident that for $\alpha<0.05$ only one panel of the lineup receives significant attention, while for $\alpha> .25$, participant attention is divided among several interesting panels of the lineup.

<<prior-predictive, echo = F, fig.cap = "Simulation of the panel selection probabilities $\\theta_i$, sorted, for different values of $\\alpha$ (left). In the right column, panel selection probabilities estimated from lineups used in past visual inference experiments; these probabilities are calculated from lineups with finite evaluations, so the calculated probabilities are not continuous. For low values of $\\alpha$, plot selections are concentrated on only a few plots, while higher values of $\\alpha$ show a wider spread of selections among more plots. The included selection probabilities from previous studies show that not all panels in each lineup are selected, even when evaluated multiple times; it is also clear that the selection probabilities are more concentrated in the turk4 study, where in some lineups only one panel was selected; in the turk14 study, at least 4 panels were selected in every lineup.", fig.width = 8, fig.height = 6, out.width = "\\textwidth" >>=
# set.seed(5019230943) # HH: too large for my machine
set.seed(519223094)

alphas <- c(.001, .02, .05, .1, .5, 1, 2, 5, 20)
# Simulate for different values of alpha
prior_sim <- tibble(alpha = alphas,
                     sel_prob = purrr::map(alpha,~ gtools::rdirichlet(100, rep(., 19)))) %>%
  mutate(
    sel_prob_ord = purrr::map(sel_prob, ~apply(., 1, sort, decreasing = T)),
    sel_prob_long = purrr::map(
      sel_prob_ord,
      ~tibble(idx = rep(1:nrow(.x), times = ncol(.x)),
              rep = rep(1:ncol(.x), each = nrow(.x)),
              prob = as.vector(.x, mode = "numeric")))
  ) %>%
  select(-sel_prob, -sel_prob_ord) %>%
  unnest(cols = c(sel_prob_long)) %>%
  arrange(alpha) %>%
  mutate(label = sprintf("alpha == %f", alpha) %>% factor(levels = sprintf("alpha == %f", alphas), ordered = T))

# Calculate thetas for different turk studies
studies_sum <- readr::read_csv("data/all-turk-studies-summary.csv")
studies_sorted <- studies_sum %>%
  mutate(study = factor(study, levels = paste0("turk", c(4:7, 10:11, 13:14)))) %>%
  filter(study %in% paste0("turk", c(4, 10, 14))) %>%
  group_by(study, pic_id, pic_name) %>%
  arrange(desc(n)) %>%
  filter(obs_plot_location != response_no) %>%
  mutate(sorted_panel = row_number(), prop = n/sum(n)) %>%
  ungroup()

theoretical_distribution_plot <- prior_sim %>%
ggplot() +
  geom_path(aes(x = idx, y = prob, group = interaction(rep, alpha)), alpha = .05) +
  facet_wrap(~label, labeller = label_parsed, nrow = 3) +
  scale_x_continuous(expression(paste("Panels, ordered by selection probability ", theta[i]))) +
  scale_y_continuous(expression(paste("Panel selection probability ", theta[i]))) + 
  ggtitle("Dirichlet simulations")

turk_studies_plot <- studies_sorted %>%
  mutate(
    study = factor(study),
    studies = c("Hofmann et al. (2012)", "Loy et al. (2016)", "Loy et al. (2017)")[as.numeric(study)]
  ) %>%
  ggplot(aes(x = sorted_panel, y = prop, group = pic_id)) +
  geom_path(alpha = .05) + facet_wrap(~studies, nrow = 3) +
  # scale_x_continuous(expression(paste("Panels, ordered by observed selection probability ", theta[i]))) +
  scale_x_continuous("") + 
  scale_y_continuous(expression(paste("Panel selection probability ", theta[i]))) + 
  theme(axis.title.y = element_blank(), axis.text.y = element_blank()) + 
  ggtitle("Vis inference studies")

gridExtra::grid.arrange(theoretical_distribution_plot, turk_studies_plot, widths = c(3, 1))
@

From figures \ref{fig:simplex} and \ref{fig:prior-predictive}, it is evident that $\alpha$ provides some information about how many panels are likely to attract participant attention.


% ------------------------------------------------------------------------------

\subsection{Lineup Administration}\label{sec:scenarios}
In addition to different types of lineups, there are different ways that an analyst can run a lineup experiment. Three possibilities are detailed below and also illustrated in \Cref{fig:scenarios}.

\begin{description}
\item [Scenario 1] $K$ different lineups are shown to $K$ independent individuals.
In this scenario, both the data and the null plots in each generated lineup are distinct from those in every other lineup.
This scenario is only practical when using purely simulated data (for both the data and null plots) or data large enough to allow for sub-sampling to generate $K$ different data plots.
Under Scenario 1, we can consider the number of target plot selections $t$ out of $K$ total evaluations, where each lineup evaluation is a Bernoulli trial; the total number of data plot evaluations can then be modeled as a Binomial distribution with selection probability $1/m$ (for a single target lineup)   \citep{majumder2013validation}.

\item [Scenario 2] $K$ different sets of null plots are shown to $K$ independent individuals; the same data plot is used in each lineup.
Alternately, $L$ sets of lineups are shown to $K > L$ individuals.
In Scenario 2, there are dependencies introduced by the reuse of the data or the lineups, providing an intermediate case between the two extremes of Scenario 1 and Scenario 3.

\item [Scenario 3] The same lineup is shown to $K$ independent individuals.
This scenario is the most common in the lineup experiments completed to date  \citep{hofmann2012graphical,roychowdhury:2012,majumder2013validation,loyAreYouNormal2015,vanderplas:2017}.
In this scenario, lineup evaluations by independent viewers are not independent because the viewers are evaluating the same combination of 19 null plots and one data plot.
Any peculiar features which arise in a null plot may cause participants to select that null plot over the data plot; it is likely that where one individual makes this choice, others might as well.
Thus, in Scenario 3, which is the most common lineup experiment scenario, it is not reasonable to assume that all panels are equally likely to be selected under the null hypothesis.
\end{description}

Regardless of the scenario, we can formulate lineups as a hypothesis test in the following manner:
\begin{description}
\item [Null hypothesis] the data plot is consistent with plots rendered from the null generating model. The probability for picking the data panel is (in distribution) the same as picking a null panel.

\item [Alternative hypothesis] the data plot is not consistent with plots rendered from the null generating model, i.e.\ based on the number of data plot selections $c_t$ the data plot is found to be interesting by participants.
\end{description}

Let $C_t$ be the random variable capturing the number of data (target) selections, $0 \le C_t \le K$. The {\emph visual $p$-value} is defined as the probability that we see at least $c_t$ evaluations in $K$ independent lineup evaluations:  
\begin{equation}\label{pvalue}
\text{visual } p-\text{value} = P(C_t \ge c_t)
\end{equation}

The exact distribution of $C_t$ depends on the scenario applied when administering the test.
Scenario 3 is the primary motivation for the model which we will develop in the next section: while we cannot consider the panels in each lineup as equally likely to be selected, we can model the individual selection probabilities using a hierarchical model.
A model for Scenario 2 would need to account for specific dependencies between the lineups; because there are many possible implementations of Scenario 2, we will leave explicit model development for those different scenarios to another paper.

<<scenarios, eval = T, echo=FALSE, out.width='\\textwidth', fig.width=12, fig.height=4, fig.cap = "Illustrations of scenarios 1 and 2. Left column shows what three different evaluators would see under scenario 1, and the right column shows what three different evaluators would see under scenario 2. In scenario 1, all nulls and data plots are different, while in scenario 2 only the nulls differ.  (Scenario 3 would have all three lineups the same.) The location of the data plot may vary from one evaluator to another.">>=
set.seed(2020)
df <- tibble(x=runif(50)) %>%
  mutate(y1 = -4*(x-0.5) + rnorm(50)) %>%
  mutate(y2 = ifelse(x<0.5, (x-0.5) + 5 + rnorm(50),
                     (x-0.5) - 5 + rnorm(50)))
l1 <- lineup(null_permute('y1'), df, n=5) %>%
  mutate(subject = "evaluator 1")
l2 <- lineup(null_permute('y1'), df, n=5) %>%
  mutate(subject = "evaluator 2")
l3 <- lineup(null_permute('y1'), df, n=5) %>%
  mutate(subject = "evaluator 3")
l <- bind_rows(l1, l2, l3)
p1 <- ggplot(l, aes(x, y1)) +
  geom_point() +
  facet_grid(subject ~ .sample) +
  theme(aspect.ratio=1,
        axis.ticks = element_blank(),
        axis.text = element_blank()) +
  xlab("") + ylab("") +
  ggtitle("Scenario 2")

df <- tibble(x=runif(50)) %>%
  mutate(y1 = -4*(x-0.5) + rnorm(50)) %>%
  mutate(y2 = ifelse(x<0.5, (x-0.5) + 5 + rnorm(50),
                     (x-0.5) - 5 + rnorm(50)))
l1 <- lineup(null_permute('y1'), df, n=5) %>%
  mutate(subject = "evaluator 1")
df <- tibble(x=runif(50)) %>%
  mutate(y1 = -4*(x-0.5) + rnorm(50)) %>%
  mutate(y2 = ifelse(x<0.5, (x-0.5) + 5 + rnorm(50),
                     (x-0.5) - 5 + rnorm(50)))
l2 <- lineup(null_permute('y1'), df, n=5) %>%
  mutate(subject = "evaluator 2")
df <- tibble(x=runif(50)) %>%
  mutate(y1 = -4*(x-0.5) + rnorm(50)) %>%
  mutate(y2 = ifelse(x<0.5, (x-0.5) + 5 + rnorm(50),
                     (x-0.5) - 5 + rnorm(50)))
l3 <- lineup(null_permute('y1'), df, n=5) %>%
  mutate(subject = "evaluator 3")
l <- bind_rows(l1, l2, l3)
p2 <- ggplot(l, aes(x, y1)) +
  geom_point() +
  facet_grid(subject ~ .sample) +
  theme(aspect.ratio=1,
        axis.ticks = element_blank(),
        axis.text = element_blank()) +
  xlab("") + ylab("") +
  ggtitle("Scenario 1")
grid.arrange(p2, p1, ncol=2)
@


\section{Significance Calculation Model Specification}


In scenario 3 the distribution for selecting any plot from a lineup assuming the null hypothesis is true is given as
$\bm \theta \sim$ Dir$(\bm\alpha)$ for constant values of $\alpha_i = \alpha > 0$ for all panels $i = 1, ..., m$. 
Consider a lineup \svp{experiment} under scenario 3, i.e.\  the same lineup containing $m$ panels, independently evaluated $K$ times.
Define $c = (c_1, ..., c_m)$ to be the counts corresponding to the set of $K$ evaluations, where $c_i$ is the number of times a participant selected panel $i$, where
$0 \le c_i \le K$ for all $1 \le i \le m$ and $\sum_i c_i = K$. 
The assumption is that each evaluator has to make a selection. In practice, evaluators get to make several selections -- these choices are incorporated into counts as fractions adding to 1. In case an evaluator makes no choice, all panel counts are increased by 1/m. For notational convenience, we will assume that all $c_i$ are integer counts. The results are immediately applicable to a non-integer number of selections, however.

The Multinomial $(K, \theta)$ distribution, with probability mass function given as:

\begin{align}\label{eqn:multinomial-pmf}
f(\bm{c}|K, \bm{\theta}) & = \frac{K!}{c_1! \cdots c_m!} \prod_{i=1}^m \theta_i^{c_i}
\end{align}

\noindent is a natural model for the counts $c_i$, where $K$ describes the number of evaluations and $\bm{\theta} = \theta_1, ..., \theta_m$ describes the probability that each panel $i=1, ..., m$ is selected.
As discussed in \Cref{sec:alpha}, the panel selection probabilities $\bm{\theta}$ can be modeled using a Dirichlet distribution with concentration parameter $\bm{\alpha} = (\alpha_1, ..., \alpha_m)$.
We do not know where a specific panel will be placed when the lineup is initially generated, so we use a symmetric  Dirichlet distribution, with $\alpha_i = \alpha, i = 1, ..., m$, that is, the concentration parameter is constant. 

The density function of the symmetric Dirichlet distribution is given as
\begin{align}\label{eqn:dirichlet-pdf}
f(\bm{\theta} \mid \alpha) & = \frac{\left(\Gamma(\alpha)\right)^m}{\Gamma(m\alpha)} \prod_{i=1}^m \theta_i^{\alpha - 1},
\end{align}
where $\Gamma(.)$ is the Gamma function defined as 
\begin{align*}
\Gamma(z) = \int_0^\infty x^{z-1}e^{-x}dx.
\end{align*}
(Note that the Gamma function is equal to the factorial function $\Gamma(n) = (n-1)!$ for integer values of $n$.)

Thus, for the observed plot selections
$(c_1, ..., c_m)$, and assuming parameter $\alpha > 0$, we specify the full model as:

\begin{align}\begin{split}\label{eqn:full-model-specification}
\bm{\theta} \mid \alpha &\sim Dirichlet(\bm\alpha)  \\
\bm{c} \mid \bm{\theta} & \sim Multinomial(\bm\theta, K)
\end{split}\end{align}

The joint distribution of $\bf c$, $\bf \theta$ is then a Dirichlet-Multinomial mixture distribution.
We will refer to this as {\bf model DM}, for brevity in later explanations.

If we were interested in the value of $\bm\theta$ and wanted to take a Bayesian approach, we would use the conjugate relationship between the Multinomial and Dirichlet distributions to get a posterior distribution of $\bm{\theta}$ given $c$ and $\alpha$ as Dirichlet$(\bm{c + \alpha})$.
However, the primary purpose is to obtain a visual $p$-value from this mixture model, which does not require that we conduct inference on $\bm\theta$. 

Rather, if we assume that the Dirichlet distribution is symmetric, corresponding to the belief that while not all panels are equally likely to be selected, there is no a priori belief that the panels systematically differ, we can conduct a test to determine whether the observed counts are consistent with a single Dirichlet parameter $\alpha$.
Restated in hypothesis testing terms, our null hypothesis is that $\bm\alpha = \alpha * (1)_{m\times 1}$.
If the lineup contains a target plot, that target plot should be overwhelmingly selected, producing counts that are not likely to occur under the assumption of symmetry in $\bm\alpha$. 


This approach differs from the method in \citet{majumder2013validation}, where the number of target plot identifications was compared with the aggregate number of null plot identifications. 
However, that approach can be considered to be equivalent to a special case of the marginal distribution of $c_i$ under DM. 
For a total of $K$ evaluations, and $0 \le c_i \le K$ target plot evaluations, the marginal model simplifies to a Beta-Binomial (BB, in later discussions):

\begin{align}\begin{split}\label{eqn:marginal-model-specification}
\theta_t \mid \alpha &\sim Beta(\alpha, (m-1)\alpha) \\
C_i \mid \theta_i & \sim Binomial(\theta_i, K),
\end{split}\end{align}

with the distribution of $\theta_i$ given $c_i$ and $\alpha$ as Beta$(c_i + \alpha, K - c_i + (m-1)\alpha)$. Here, $i$ is the index of the target plot, with null panel selections considered in aggregate.

Examining the parameters of either the full (\Cref{eqn:full-model-specification}) or marginal model (\Cref{eqn:marginal-model-specification}) specifications tells us that $\alpha$ provides the equivalent of pseudo-observations for each plot. That is, the effect of $\alpha$ is equivalent to adding $\alpha$ identifications to each panel in the lineup.
When $\alpha$ is small, these pseudo-observations have relatively little influence, but when $\alpha$ is large, the pseudo-observations can quickly dwarf any information provided by the data.
This is particularly true for the marginal BB model, where the equivalent of $(m-1)\alpha$ pseudo-observations are added. 
A lineup might be evaluated between 10 and 30 times, and with a $m=20$ panel lineup, even $\alpha = 1$ can easily dominate any signal present in the participant selection data.


As discussed in \Cref{sec:alpha}, the value of $\alpha$ also determines how many panels will have a relatively high selection probability $\theta_i$ (and how many panels will have $\theta_i\approx 0$). 
This means that $\alpha$ also determines how many panels in a lineup are likely to attract participant interest.
The next section explores the impact and interpretation(s) of $\alpha$ in the context of statistical lineups.


\subsection{Effect of \texorpdfstring{$\alpha$}{alpha} on Lineup Scenarios}\label{sec:scenario-alpha}

% Argument outline:
% 1. alpha provides information about how many panels attract participant attention
% 2. Participant attention is a function of visual difference perception, but it is also a function of the data generation method
% 3. Compare Scenario 1 vs. Scenario 3
%     a. Scenario 1: new panels with each new lineup; no dependencies between panels. Every panel is equally interesting (at least, under $H_0$). This corresponds to infinite $\alpha$, or, more explicitly, to $\theta = 1/m$.
%     b. Scenario 3: each participant evaluates the same lineup, so the relationship between the interest level of each panel is constant. This corresponds to finite $\alpha$, and generally, $\alpha \lt 1$, that is, all panels are not equally interesting.
% 4. The binomial model is effective in scenario 1, because we do not have any information about $\theta_i$ shared between subsequent lineup evaluations. In Scenario 3, however, we do, and thus, it becomes important to model that difference using the mixture model we propose in this paper.
% 5. If we work under Scenario 3, we should expect $\alpha$ to differ based on the data generation model and based on the form and aesthetic presentation of the plot. Thus, for each experiment, we need to estimate $\alpha$.

Under Scenario 1, each participant sees an entirely different lineup: the data plot(s) and null plots are exchanged between each participant evaluation.
Thus, while there may be differences in the visual interest of each panel in any one lineup, these differences do not carry over to the next lineup.
While all panels in a single generated lineup may not be equally likely to be selected, we do not have any information or ability to estimate or quantify these differences.
In fact, in Scenario 1, it does not make sense to track any information beyond whether or not the data panel was selected: each evaluation is in effect a separate, independent Bernoulli trial.
Our selection of $\alpha$ under this scenario goes beyond what might be considered a non-informative $\alpha=1$ (corresponding to uniformly distributed $\theta_i$ over the $m-1$ simplex).
Instead, because we are averaging over all panels which could be generated by the data and null models (as both the data plot and the null plots are exchanged every time), we can claim that every plot is strictly equally likely to be selected under $H_0$. 
As shown in \Cref{fig:vis-p-val-sensitivity-initial}, when $\alpha \rightarrow\infty$, the \dc{BB} model converges asymptotically to the Binomial model (with $\theta = 1/m$) proposed in \citet{majumder2013validation}.

<<vis-p-val-sensitivity-initial, fig.cap = "Sensitivity of visual p-value to selection of $\\alpha$ under the marginal beta-binomial model. Corresponding values for the binomial model are shown on the right side of the plot; as $\\alpha \\rightarrow\\infty$, the beta-binomial $p$-values converge to the binomial model $p$-value.", fig.width = 8, fig.height = 5, out.width = "\\textwidth">>=

alphas <- 10^(seq(-3, 2, by = .01))
# data_breaks <- c(1:5, 6, 8, 10, 15, 20)
data_breaks <- c(1, 2, 3, 5, 7, 10, 15, 20, 25, 30)
K <- 30

color_pal <- RColorBrewer::brewer.pal(length(data_breaks), "Paired")

breaks <- c(10^seq(-3, 2, 1), 5*10^seq(-3, 1, 1)) %>% sort()
minor.breaks <- c(2.5, 7.5) * rep(10^seq(-3, 1, 1), each = 2)
labels <- sprintf("%.3f", breaks) %>% gsub("\\.?0{1,}$", "", .)

breaks <- c(breaks, 150)	
labels <- c(labels, "∞")

pv <- tidyr::crossing(alpha = alphas, C = data_breaks, K = K) %>%
  mutate(p = vis_p_value(C, K, alpha))
pv2 <- tidyr::crossing(C = data_breaks, K = K) %>%
  mutate(p = purrr::map2_dbl(C - 1, K, pbinom, prob = 1/20, lower.tail = F))

ggplot(pv, aes(x = alpha, y = p, color = factor(C), group = factor(C))) +
  annotate("segment", x =150, xend = 150, y = -Inf, yend = .8, color = "grey40", alpha = .5) +
  annotate("text", x = 150, y = .8,
           label = "Binomial", vjust = 0, hjust = .75) +
  annotate("segment", x = 1, xend = 1, y = -Inf, yend = .8, color = "grey40", alpha = .5) +
  annotate("text", x = 1, y = 0.8,
           label = expression(paste("θ ~ Uniform on m-1 simplex")), vjust = 0, hjust = .5) +
  geom_line(size = 0.75) +
  geom_point(aes(x = 150, y = p, color = factor(C), shape = "Binomial\np-value"), data = pv2) +
  scale_y_continuous("Visual p-value") +
  scale_x_log10(expression(alpha), breaks = breaks, labels = labels, minor_breaks = minor.breaks) +
  scale_color_manual(paste0("# Data Panel\nIdentifications\n(K =", K, ")"), values = color_pal) +
  scale_shape_discrete("") +
  geom_hline(yintercept = 0.05, color = "grey40") +
  guides(color = guide_legend(override.aes = list(shape = NA), order = 1),
         shape = "none") +
  theme_bw() +
  theme(legend.position = c(0, 1), legend.justification = c(0, 1), legend.background = element_blank(), legend.box = "horizontal")
@


Under Scenario 3, however, each participant sees the same lineup, with the same data plot(s) and null plots.
In this scenario, we have enough information that we can model the $\theta_i$ across different lineup evaluations.
Because the perceptual mechanisms which determine visual interest are shared across participants, and the same null plots and data plot(s) are used, we must allow $\theta_i$ to vary.
The DM model (\Cref{eqn:full-model-specification}) provides this flexibility through the introduction of the parameter $\alpha$.
The general formula for calculating a visual $p$-value under the BB model appropriate for use in Scenario 3 is:
\begin{align}\label{eqn:beta-binomial}
p\text{-value} = P(C\geq c_i) = \sum_{x = c_i}^{K} \binom{K}{x} \frac{1}{B(\alpha, (m-1)\alpha)}\cdot B(x+\alpha, K-x+(m-1)\alpha)
\end{align}
where $c_i$ is the number of times the data panel was picked in $K$  evaluations of the lineup. $B(., .)$ is the Beta function defined as:
\begin{align*}
B(a, b) = \int_0^1 t^{a-1} \cdot (1-t)^{b-1} dt \vspace{1in} \text{ where } a,b > 0.
\end{align*}
The derivation is in \Cref{app:pvalue}.

The visual $p$-value calculation using \Cref{eqn:beta-binomial} is dependent on the value of $\alpha$.
We know from past studies \citep{hofmann2012graphical,roychowdhury:2012,zhaoMindReadingUsing2013,roychowdhury:2013,tengfei:2013,loyDiagnosticToolsHierarchical2013,majumderHumanFactorsInfluencing2014,loy:2015,loyAreYouNormal2015,loy2016variations,vanderplas:2017,loyModelChoiceDiagnostics2017} that only a few lineup panels attract attention, even if all of the panels in a lineup are null plots.
Combining this observation with \Cref{fig:prior-predictive}, we would expect that $\alpha \ll 1$.

The value of $\alpha$ would be expected to vary based on various factors: the null generating model, type of plot, and other aesthetic choices,  all of which could affect the visual distinctiveness of the null and actual data.
The right side of \Cref{fig:prior-predictive} shows three studies with different distributions of $\theta_i$ that have characteristics corresponding to different $\alpha$ values.
To calculate visual $p$-values for lineups evaluated under Scenario 3, we must match the underlying data generation method and visual evaluation processes with an appropriate value of $\alpha$.

As it is difficult to design a null plot generating method which will result in a specific $\alpha$ value, in practice, we will need to select an appropriate $\alpha$ for a predetermined null plot generating model.
The selected $\alpha$ modulates the calculated visual $p$-value, as shown in \Cref{fig:vis-p-val-sensitivity-initial}: when $\alpha$ is low, there are likely one or more visually distinctive null plots, making it difficult to attribute data panel selections to definitive visual differences between the null and data plots.
When $\alpha$ is relatively high, however, there are likely to be more null plots that attract visual attention; in this situation, it is very easy to determine whether the data panel is visually distinct compared to the null panels.
Clearly, the choice of $\alpha$ is critical.

\section{Estimation of \texorpdfstring{$\alpha$}{alpha}}\label{sec:est-alpha}

While it is generally possible to use maximum likelihood to estimate $\alpha$ \citep{sirt, minka} directly based on proportions observed from panel selections in a lineup, these approaches fail because of the large number of observed zeroes. 
When participants select only a small subset of interesting null panels, there are naturally many panels that have no selections.
The many zeroes are even more pronounced when $\alpha$ values are small and only one or two null panels attract participant interest.
The consequence is that these methods for estimation of $\alpha$ are most likely to fail in precisely the region of the parameter space where lineup experiments typically operate.

Instead, in this section, we propose a method for \emph{visual} estimation of $\alpha$ which accommodates zero-count values and a numerical method for estimating $\alpha$ based on the results of Rorschach lineup based testing. %\dc{These are described here.}
Both the visual and numeric methods for estimating $\alpha$ leverage the expected number of "interesting" panels in a lineup.

\begin{definition}{$c$-interesting}\label{interesting}\newline
We define lineup panel $i$ to be $c$-interesting if $c$ or more participants selected the panel as the most different. 
\end{definition}
This definition gives us an objective way to let evaluators determine what is interesting.
The threshold $c$ does not have to be an integer value.

Using this definition, we \svp{also} define the random variable $Z_c$, the number of panels which \svp{are $c$-interesting} in $K$ evaluations of an $m$-panel lineup.
The expected number of panels selected at least $c$ times, $E[Z_c]$, is provided in \Cref{eqn:ev-panel}. The derivation of $E[Z_c]$ from \Cref{eqn:full-model-specification} is provided in \Cref{app:expected}.

\begin{equation}\label{eqn:ev-panel}
E[Z_c(\alpha)] = \frac{m}{ B(\alpha, (m-1)\alpha)} \cdot \sum_{x=\lceil c \rceil}^K \binom{K}{x} B(x+\alpha, K-x+(m-1)\alpha).
\end{equation}

Note that $c$-interestingness and $Z_c$ both depend on the experimental conditions - the number of times a lineup is evaluated, and the number of panels in the lineup. 
While it would be reasonable to normalize the definition of $c$-interesting by $K$ or $m$, this would imply that we can compare across different scenarios.
Unfortunately, because we are usually dealing with relatively small values of $K$ and $m$, even when we normalize counts, we still have a set of discrete quantities.


In a lineup experiment, we typically compare the selections of the data panel relative to the \emph{aggregate} selections of null panels, using the marginal BB model.
While this makes the calculations much simpler, because there is no need to keep track of the locations and selections of individual null plots, it does throw away some information; namely, the distribution of participant interest in the null panels.
The proposed estimation methods are predicated on utilizing that discarded information to estimate $\alpha$. % This paragraph actually applies to both methods.

We will first discuss the visual and numerical methods for estimation of $\alpha$ from lineup evaluations, and then discuss one consequence of these methods for improved lineup diagnostics.

\subsection{Estimation using null choices in a one target lineup}\label{sec:vis-estimation-alpha}

There have been several explorations of the use of visual statistics as a supplement or an alternative to statistical inference~\citep{mostellerEyeFittingStraight1981, correllRegressionEyeEstimating2017, LAWRENCE1989172, meyerEstimatingCorrelationsScatterplots1992}.
Visual estimates of correlation and linear regression are known to differ systematically from the numerical estimates, but not because the visual system is inaccurate or misleading.
Instead, estimates derived visually tend to discount the effect of outliers, producing a more robust estimate of the statistical quantity of interest.
In this application, we expect that visual estimation of $\alpha$ based on the expected number of "interesting" panels will produce a more robust estimate of $\alpha$ than the numerically unstable maximum likelihood estimates.

\Cref{fig:prior-predictive} illustrates that $\alpha$ is directly related to the number of panels that are selected as interesting by participants.
The estimation of $\alpha$ is based on the relationship between $\alpha$ and the number of expected $c$-interesting panels as developed in the previous section.
\Cref{fig:alpha-sim-curve} illustrates this relationship for the example of $K=30$ evaluations of 19 null panels: 
the blue line corresponds to the expected number of 1-interesting null panels $Z_1$ as given in \Cref{eqn:ev-panel}.
Each point in the figure is based on 10 simulations of a lineup of size $(K, m)$.

This provides some variability around how many times each panel was selected, but the number of panels which attract participant attention is fairly consistent across multiple simulations.
Light grey bands corresponding to four levels of `interestingness' are shown in \Cref{fig:alpha-sim-curve}: we consider situations with 2-3, 4-5, 6-7, and 8+  null panels that were selected at least once. \Cref{fig:alpha-sim-curve} then can be used for visually estimating $\alpha$ by executing a reverse lookup of the rate parameter based on the number of interesting null panels.



A natural threshold for sufficient participant interest is $c = K/m$; that is, setting the threshold $c$ to be greater than the expected number of selections under the Binomial model assumption that all panels have selection probability $1/m$. 
Using null panel selections hinges on a property of the Dirichlet distribution: when one category is removed from consideration, the remaining categories still maintain a reduced-dimension Dirichlet distribution with the same parameter $\alpha$.
A discussion of this property and its application to visual inference is provided in \Cref{neutrality}.


<<alpha-sim-curve, cache = F, fig.width = 8, fig.height = 4, out.width = "\\textwidth", fig.cap = "Average number of panels selected more than once for a range of $\\alpha$ values. Each point represents 10 simulations of lineups with $K=30$ evaluations. The line in blue shows the expected number of panels selected more than once as given in \\Cref{eqn:ev-panel}. Bands are shown in alternating grey and white corresponding to a discretized heuristic for selection of $\\alpha$ when $K=30$.">>=
set.seed(2501072)
interest_threshold <- 2
alphas <- 10^(seq(-3.5, 2, by = .01)) # seq(0.001, 0.5, by = .001)

number_panels <- function(alpha, c=1, m = 20, K=30) {
  x <- ceiling(c):K
  summation <- choose(K, x) * beta(x + alpha, K - x + (m - 1)*alpha)

  m/beta(alpha, (m - 1)*alpha)*sum(summation)
}

sim_interesting_panels <- function(c = m/K, N_points = 5, m = 19, K = 30, point_avg_sims = 10, alphas = alphas) {
  # Each point (of N_points) is an average of point_avg_sims separate lineups
  # First, generate all of the lineups and count the number of interesting panels
  df <- tibble(alpha = alphas,
               plot_sels = purrr::map(alpha, sim_lineup_model,
                                      N = N_points*point_avg_sims, K = K),
               interesting_panels = purrr::map(
                 plot_sels,
                 ~tibble(n_interesting = colSums(.x >= c),
                         rep = 1:ncol(.x) - 1))
  ) %>% unnest(interesting_panels)

  # Then, average the panels together a bit
  df2 <- df %>%
    mutate(point_num = (rep - (rep %% point_avg_sims))/N_points) %>%
    group_by(alpha, point_num) %>%
    summarize(n_interesting = mean(n_interesting))
}

# Create polygons that connect a function to the y and x axes
lag_polys <- function(data) {
  x = data$x
  y = data$y
  minval = 10e-8
  bind_rows(
  tibble(
    x = c(minval, x[1], x[2], minval, minval),
    y = c(y[1], y[1], y[2], y[2], y[1]),
    type = "h"
  ),
  tibble(
    x = c(x[1], x[2], x[2], x[1], x[1]),
    y = c(-Inf, -Inf, y[2], y[1], -Inf),
    type = "v"
  ))
}

# Get theoretical function
model_df <- tibble(alpha = alphas,
                   n_sel_plots = alphas %>% map_dbl(number_panels, c = interest_threshold)
)

# Get simulated data
prior_pred_mean <- sim_interesting_panels(c = interest_threshold, alphas = alphas) %>%
  arrange(alpha) %>%
  mutate(label = sprintf("alpha == %f", alpha) %>%
           factor(levels = sprintf("alpha == %f", alphas), ordered = T))

# Get possible breakpoints for alphas and panels
panel_sel_breaks <- c(1.5, 3.5, 5.5, 7.5)
possible_breakpoints <- model_df %>%
  filter(round(n_sel_plots - floor(n_sel_plots), 1) == .5 |
           round(n_sel_plots - floor(n_sel_plots), 1) == 0) %>%
  mutate(tmp = floor(n_sel_plots*2)/2, dist = abs(n_sel_plots - tmp)) %>%
  group_by(tmp) %>%
  filter(dist == min(dist)) %>%
  filter(tmp %in% panel_sel_breaks) %>%
  ungroup() %>%
  mutate(band = floor(.5 + row_number()/2))

# polygons for the grey-and-white bands
bands <- possible_breakpoints %>%
  rename(x = alpha, y = n_sel_plots) %>%
  nest(data = -band) %>%
  mutate(polys = purrr::map(data, lag_polys)) %>%
  unnest(polys)

# minor break points
mb <- crossing(x = c(2.5, 5, 7.5), y = 10^(seq(-4, 5))) %>%
  mutate(z = x*y) %>%
  `[`("z") %>%
  unlist() %>%
  as.numeric

# text labels for polygon bands
band_label_pos <- panel_sel_breaks + 1 # this is a good default but may need customizing
band_labels <- tibble(
  y = band_label_pos,
  x = min(alphas),
  label = c("2-3", "4-5", "6-7", "8+ Interesting panels")
)

# Finally, the actual plot
ggplot() +
  geom_polygon(aes(x = x, y = y, group = interaction(type, band)), data = bands, fill = "grey", alpha = 0.5) +
  geom_point(aes(x = alpha, y = n_interesting),data = prior_pred_mean, alpha = .05) +
  geom_line(aes(x = alpha, y = n_sel_plots), data = model_df, color = "blue", size = 1) +
  scale_x_log10(name = expression(alpha), breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000),
                minor_breaks = mb,
                labels = 10^(seq(-3, 4))) +
  geom_text(aes(x = x, y = y, label = label), data = band_labels, hjust = 0, color = "grey30") +
  coord_cartesian(xlim = range(alphas)) +
  scale_y_continuous(name = "Average number of panels with >1 selection", breaks = 1:100)

@

The proposed visual estimation process is conducted using a simulation from the DM model in \Cref{eqn:full-model-specification}.
Furthermore, we will assume for this demonstration that the lineup experiment we are planning to conduct has a standard ($m = 20$ panel, $m_0 = 19$ null plot) lineup with $K_0=30$ null panel selections out of $K=40$ evaluations.

\begin{description}
\item[Identify the number of $c$-interesting null panels in the lineup of interest.] Consider \Cref{fig:alpha-sim-curve}. Generally, this figure needs to be adjusted to match the number of null panels,   $m_0$,  and the corresponding number of null panel selections, $K_0$. The \texttt{alpha\_from\_data\_lineup()} function in the \texttt{vinference} package (available at https://github.com/heike/vinference) can be used to create this plot given values of $c$, $m_0$, and $K_0$.
% \hh{XXX should we provide an R function?}

{\it Aside:} \hh{this should be discussed in full, but not here - maybe in the discussion? } If only one null panel is selected more than $c$ times, and there are a reasonable number of null panel selections overall, this may be a sign that the null plot generation method is unsuitable.  The case where only one null panel is selected is handled separately; discussion of this case is deferred to \Cref{sec:bad-null}. For now, it is sufficient to know that this is why we have created a separate band for the case where only one null plot attracts significant participant interest.

\item [Locate a $y$-axis range around the observednumber of $c$-interesting null panels in the data] Using the simulated selected panel counts, on the vertical axis locate a band around the number of $c$-interesting null panels in the lineup. For instance, if there were three null panels with more than $c$ selections, we would use a range of 2-3 panels.

Using  a range instead of a single point estimate is intended to account for some of the variability resulting from the use of a single lineup with non-deterministic $K_0$ null panel evaluations.
In a simulation, it is easy to increase the number for simulations $N$ until  results no longer change substantially; this is expensive and much more time consuming when using human evaluations and setting $K$ instead of $K_0$ in the experimental design.

\item [Select an $\hat\alpha$ value corresponding to the selected band and calculate the visual p-value.] Using the chosen $\hat\alpha$ value, calculate a visual p-value using \Cref{eqn:beta-binomial}. Assess the sensitivity of this p-value calculation to the choice of $\alpha$ values within the selected band. A plot such as the one shown in \Cref{fig:vis-p-val-sensitivity-initial2} may be helpful when assessing the sensitivity of the visual $p$-value to different values of $\alpha$ in each band. \Cref{fig:vis-p-val-sensitivity-initial2} is a segmented version of \Cref{fig:vis-p-val-sensitivity-initial}; each panel corresponds to the bands of interesting panel counts selected in step 1. Locate the relevant panel of the plot, and, using the number of target panel selections, determine whether the visual $p$-value calculation is conclusive for every $\alpha$ in the panel. 

For instance, if our selected lineup contained 3 interesting null panels, $\hat\alpha = 0.075$ approximately corresponds to $Z_c = 3$; we should compare to $\alpha = 0.01$ and $\alpha=0.09$ (the approximate outer range of $\alpha$ for our band) to get a sense of the sensitivity of our $p$-value to $\alpha$. Examining the second panel of \Cref{fig:vis-p-val-sensitivity-initial2}, which corresponds to 3 interesting null panels, we see that p-values will significant at the 0.05 level when the number of data panel selections is at least 20; if there are fewer than 15 data panel selections, then p-values will not be significant at the 0.05 level. In our example, we have 10 target selections ($K = 40, K_0 = 30$), corresponding to a visual $p$-value that will not be significant for any range of $\alpha$ in our selected band.
\end{description}

<<vis-p-val-sensitivity-initial2, fig.cap = "Sensitivity of visual $p$-value to selection of $\\alpha$ under the beta-binomial model, for a $m=20$ panel lineup with $K=40$ evaluations. Data picks which result in ambiguous results under the discretized bands of $\\alpha$ values are highlighted in red. At any particular $\\alpha$ level, there are only a few values which result in an inconclusive results.", fig.width = 8, fig.height = 5, out.width = "\\textwidth">>=
alphas <- 10^(seq(-3, 2, by = .01))
data_breaks <- 1:40
K <- 40
m <- 20

cuts <- c(-Inf, .02, .05, .1, .35, 1, 10, Inf)
cuts <- c(-Inf, possible_breakpoints$alpha, Inf)
labels <- c("0-1 Interesting null panels", "2-3 Interesting null panels", "4-5 panels", "6-7 panels", "8+ Interesting null panels")

pv <- tidyr::crossing(alpha = alphas, C = data_breaks, K = K) %>%
  mutate(p = vis_p_value(C, K, alpha)) %>%
  mutate(
     panel = cut(alpha, breaks = cuts, labels = labels)
     ) %>%
   group_by(C, K, panel) %>%
   mutate(
        type = ifelse(
          all(p > 0.05), "indicates no significance",
          ifelse(all(p < 0.05), "indicates significance",
          "is inconclusive")
         )
   )

labels_data <- pv %>%
   group_by(panel) %>%
   filter(
     alpha == max(alpha)
   )

ggplot(pv, aes(x = alpha, y = p, group = factor(C))) +
  facet_wrap(~panel, scales = "free") +
  geom_line(size = .8, aes(colour = type)) +
  scale_y_continuous("Visual p-value") +
   scale_x_log10(expression(alpha),
                 expand = expansion(mult = c(0.05,0.25))) +
  geom_hline(yintercept = 0.05, color = "black") +
  ggrepel::geom_label_repel(
     data = labels_data %>% group_by(panel, type) %>%
        filter(((type == "indicates no significance") & (C == max(C))) | ((type == "indicates significance") & (C == min(C)))),
     aes(label = C), color = "grey30", hjust = 0, nudge_x = 0.05, size = 3, alpha = 0.8,
         min.segment.length= 0, label.padding = 0.1, direction = "y") +
   theme_bw() +
   geom_label(label = paste0("# data picks\n (out of K = ", K, ")"),
              aes(x = .0010, y = .09), size = 3,
         data = labels_data[1,], hjust = 0, vjust = 1) +
   scale_colour_manual("Number of data picks ...",
                       values = c("grey70", "grey40", "red")) +
   theme(legend.position = c(.99, 0.45), legend.justification = c(1, 1))

@


In \Cref{fig:vis-p-val-sensitivity-initial2}, we show the implications of the visual selection method for each band of $\alpha$ values in terms of the number of target plot selections necessary to achieve statistical significance at the $p=0.05$ level.
Due to the discretization of the expected number of panels with more than $c$ selections, each range of $\alpha$ values is ambiguous for one or more potential data panel selection counts; these values are shown in red, with labeled thresholds for non-significance and significance.
In the unfortunate situation where the estimated $\hat\alpha$ produces an inconclusive result, the experimenter has two options.
The inexpensive, conservative approach is to declare any inconclusive results to be \dc{non-significant}, in effect using the smallest $\alpha$ value corresponding to the approximate number of panels selected.
Alternately, the experimenter could use the method described in \Cref{sec:rorschach-est-alpha} to produce a more precise $\hat\alpha$ which would provide definitive results, at the cost of conducting a secondary study.


By estimating $\alpha$, we produce visual $p$-values calibrated based on the specific null plot generation method.
In most cases, we get the improved calibration for free, because we can obtain this information from the null panels in one or two target lineups.
Occasionally, either because the signal in the data plot is too strong, or because the visual estimation method for $\alpha$ produces a range of $p$-values that are inconclusive given the number of target plot selections, we may need to invest additional effort to generate a precise $\hat\alpha$ estimate numerically.

In the case where a tested lineup has selections which overwhelmingly favor the data plot, we have an interesting dilemma: the lineup is likely significant (based on the overwhelming evidence that the data plot attracts the most visual interest), but we cannot estimate $\alpha$ because there are insufficient null panel selections.
In this case, estimation of $\alpha$ will depend on the creation of a Rorschach lineup (that is, a lineup consisting entirely of plots generated under the null hypothesis).
This process is described in more detail in \Cref{sec:rorschach-est-alpha}.

As it is rare for the data plot to be the only panel to attract participant attention, we can usually recover information about $\alpha$ from the null panels in the same lineup.
This is a more efficient use of participant time and experimenter resources, as we do not have to ask participants to evaluate two separate lineups to determine the visual $p$-value for a plot.

\subsection{Estimation using Rorschach lineups}\label{sec:rorschach-est-alpha}

\Cref{eqn:ev-panel} can be used to generate a more precise estimate of $\alpha$ with a secondary study consisting of one or more Rorschach lineups evaluated $K$ times each.
This estimation method requires the following steps:
\begin{description}
\item [Experimentally evaluate $L$ $m$-panel Rorschach lineups $K$ times each.] To ensure validity of the estimated $\alpha$, the Rorschach lineups should use the same null data generating mechanism used in the standard lineup. It is also important that $K$ be the same for each lineup which is evaluated, as \Cref{eqn:ev-panel} depends on $K$.

\item [Calculate the mean number of $c$-interesting panels in the Rorschach lineups.] This empirical estimate of $E[Z_c]$ will be used in combination with \Cref{eqn:ev-panel} to estimate $\alpha$.

\item [Numerically determine $\hat\alpha$ using $\overline Z_c$ and \Cref{eqn:ev-panel}.]The reference distribution is one with $m=m_0$ null panels, because of the use of Rorschach lineups, in contrast to the reference distribution in \Cref{sec:vis-estimation-alpha}. 
\end{description}

The \texttt{alpha\_from\_null\_lineup()} function in the \texttt{vinference} package (available at https://github.com/heike/vinference) can be used to estimate $\hat\alpha$ given values of $\overline Z_c$, $c$, $m_0$, and $K_0$.

Note that if on average only one null panel is selected, the estimated $\hat\alpha$ is not reliable: \Cref{eqn:ev-panel} does not have the sensitivity to differentiate the results of a lineup with $\hat\alpha = 0.001$ from the results of a lineup with $\hat\alpha = 0.01$; this is also apparent on the left side of \Cref{fig:alpha-sim-curve}. When the estimated $\hat\alpha$ is extremely small, this is a signal that there is a problem with the null plot generation method; this case is described in more detail in \Cref{sec:bad-null}.

This approach could also be used in situations where a precise $\hat\alpha$ is necessary, as it has greater numerical precision than the visual estimation method proposed in \Cref{sec:vis-estimation-alpha}. An intermediate solution might be to include a few Rorschach lineups in the design of a lineup experiment, so that enough data is obtained from the Rorschach lineups to estimate $\alpha$, without requiring the overhead of an additional study or a large number of additional participant evaluations. In this compromise approach, both the visual method and the numerical method could be used: the first to obtain an approximate interval, and the second to justify a specific value of $\alpha$. 

\subsection{Detecting inadequate null generation methods}\label{sec:bad-null}
When only one null plot in a Rorschach lineup is visually interesting, we encounter two problems: (1) $\hat\alpha$ cannot be estimated with precision, because many different $\alpha$ values below $\alpha\approx 0.025$ could give rise to a situation where only one panel is visually interesting; (2) any one-target lineup may be confounded if the data plot replaces the interesting null plot.
If the interesting null is replaced by the data plot, we would expect that the count values would look similar to those produced when evaluating the Rorschach lineup, but even if it is not, the distribution of counts might not differ enough to show statistical significance when $\hat\alpha$ is small.
The lineup hypothesis testing method is predicated on the ability to visually distinguish a Rorschach lineup from a lineup with a data target, so a method that sporadically generates overwhelmingly interesting null plots provides insufficient grounds for rejection of the null hypothesis.

Another interpretation of an extremely low $\hat\alpha$ value is to cast  suspicion on the null generating process. Null generating methods shouldn't inadvertently generate highly variable numbers of interesting nulls; rather one expects them to be uniformly dull.
An example is described in \citet{vanderplas:2017}, where the generated null plots occasionally lacked important features critical to the study that made them inadvertently visually different.
Estimation of $\alpha$ provides us with a way to screen for bad null plot generation methods, in addition to providing more accurate estimates of visual $p$-values.

We can see from \Cref{fig:vis-p-val-sensitivity-initial} that the proposed method will generate $p$-values that are more moderate than the simpler Binomial model that does not involve the extra step of estimating $\alpha$. 
In the next section, we explore the practical implications of this method by examining actual lineups that were tested experimentally, and the visual $p$-values calculated using each method.

\section{Example}
<<example-setup, cache = T, include = F>>=
options(digits = 3)

exdat <- readr::read_csv("data/turk6_60_96_1_8.csv")

# Generate a null lineup with the same 19 null plots + one extra null
exdat2 <- mutate(exdat, group = ifelse(.sample == 8, sample(group, n()), group))

studies_sum <- readr::read_csv("data/all-turk-studies-summary.csv")

# Number of null plots selected (out of 19)
turk6df <- filter(studies_sum, study == "turk6") %>%
  group_by(pic_name) %>%
  mutate(n_tot = sum(n)) %>%
  filter(n >= 2/n_tot*36 & response_no != obs_plot_location) %>%
  summarize(n_tot = unique(n_tot), n = n())

intercept <- mean(turk6df$n/turk6df$n_tot*36)

exdf <- filter(studies_sum, pic_name == "file147f93f77aae9.png") %>%
  filter(n != 0) %>%
  rename(.sample = response_no)

exdf_targetcount <- sum(filter(exdf, .sample == obs_plot_location)$n)
exdf_nullcount <- sum(filter(exdf, .sample != obs_plot_location)$n)
total_n <- exdf_targetcount+exdf_nullcount
old_vispval <- vis_p_value_orig(exdf_targetcount, total_n)
new_vispval <- vis_p_value(exdf_targetcount, total_n, alpha = .174)

new_vispval_low <- vis_p_value(exdf_targetcount, total_n, alpha = .115)
new_vispval_high <- vis_p_value(exdf_targetcount, total_n, alpha = .251)
@
This is a reassessment of the $p$-value computed for a lineup (\Cref{fig:example-plot}) used in \citet{majumder2013validation}. 
Using the DM model, computed visual $p$-values are more conservative than the previously computed using the Binomial model proposed in that work.
The responses from evaluators were somewhat ambiguous: the data plot in panel 8 was selected most frequently by participants (14 times), but in total, null plots were selected more frequently than the target plot (22 times). In particular, the null plot in panel 4 attracted 10 selections.
Using a Binomial model for the evaluation this plot  produces a visual $p$-value on the order of $10^{-9}$, %\Sexpr{old_vispval}
which seems extremely small given that there were more null plot selections than data plot selections.

<<example-plot, dependson = "example-setup", echo = F, fig.width = 5, fig.height = 5, fig.align="center", out.width = ".8\\textwidth", fig.cap = "A lineup designed to test the utility of boxplots for detecting distributional differences. Selection counts are provided in light grey on top of each panel which was selected at least once. The data panel is highlighted in dark grey (panel 8).">>=
ggplot(exdat, aes(x = group, y = vals, fill = factor(group))) + 
  geom_boxplot() + 
  facet_wrap(~.sample) + 
  theme_bw() + 
  theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank()) + 
  scale_fill_discrete(guide = "none") +
#  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf), fill="white", alpha = 0.003) +
  annotate("rect", xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf, fill="white", alpha = 0.5) +
  geom_text(aes(x = 1.5, y = 0.830, label = n), nudge_x = 0, nudge_y = 0, hjust = .5, vjust = .5, inherit.aes = F, size = 20, data = exdf, color = "grey70", alpha =0.8) +
  geom_rect(aes(xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf), fill="black", alpha = 0.5, inherit.aes=FALSE, data = exdf %>% filter(.sample==8)) +
  geom_text(aes(x = 1.5, y = 0.830, label = n), nudge_x = 0, nudge_y = 0, hjust = .5, vjust = .5, inherit.aes=FALSE,
  data = exdf %>% filter(.sample==8),
  size = 20, color = "grey90", alpha =0.7)

# ggplot(exdat, aes(x = group, y = vals, fill = factor(group))) +
#   geom_text(aes(x = Inf, y = -Inf, label = n), nudge_x = -1, nudge_y = 1, hjust = 1.2, vjust = -0.2, inherit.aes = F, size = 10, data = exdf, color = "grey80") +
#   geom_boxplot() +
#   facet_wrap(~.sample) +
#   theme_bw() +
#   theme(axis.text = element_blank(), axis.title = element_blank(), axis.ticks = element_blank()) +
#   scale_fill_discrete(guide = "none")


@
Under the DM model in \Cref{eqn:full-model-specification}, the first step is to assess the expected number of null panels which  attract attention: six of 19 null plots were selected from the lineup at least once. The corresponding simulation in \Cref{fig:example-alpha-curve} suggests a value of $\alpha$ between 0.1 and 0.25.

<<example-alpha-curve, fig.cap = "Number of expected panels with at least one selection in the 22 null plot selections  for the lineup shown in \\Cref{fig:example-plot}.", fig.width = 8, fig.height = 4, out.width = "\\textwidth">>=

interest_threshold <- 1
alphas <- 10^(seq(-3.5, 2, by = .01)) # seq(0.001, 0.5, by = .001)

# Get theoretical function
model_df <- tibble(alpha = alphas,
                   n_sel_plots = alphas %>%
                     map_dbl(number_panels, K = 22, m = 19, c = interest_threshold)
)

# Get simulated data
prior_pred_mean <- sim_interesting_panels(c = interest_threshold, K = 22, m = 19, alphas = alphas) %>%
  arrange(alpha) %>%
  mutate(label = sprintf("alpha == %f", alpha) %>%
           factor(levels = sprintf("alpha == %f", alphas), ordered = T))

# Get possible breakpoints for alphas and panels
panel_sel_breaks <- c(5, 7)
possible_breakpoints <- model_df %>%
  filter(round(n_sel_plots - floor(n_sel_plots), 1) == .5 |
           round(n_sel_plots - floor(n_sel_plots), 1) == 0) %>%
  mutate(tmp = floor(n_sel_plots*2)/2, dist = abs(n_sel_plots - tmp)) %>%
  group_by(tmp) %>%
  filter(dist == min(dist)) %>%
  filter(tmp %in% panel_sel_breaks) %>%
  ungroup() %>%
  mutate(band = floor(.5 + row_number()/2))

panel_selected <- 6
selected_breakpoints <- model_df %>%
  filter(round(n_sel_plots - floor(n_sel_plots), 1) == .5 |
           round(n_sel_plots - floor(n_sel_plots), 1) == 0) %>%
  mutate(tmp = floor(n_sel_plots*2)/2, dist = abs(n_sel_plots - tmp)) %>%
  group_by(tmp) %>%
  filter(dist == min(dist)) %>%
  filter(tmp %in% panel_selected) %>%
  ungroup() %>%
  mutate(band = floor(.5 + row_number()/2))



# polygons for the grey-and-white bands
bands <- possible_breakpoints %>%
  rename(x = alpha, y = n_sel_plots) %>%
  nest(data = -band) %>%
  mutate(polys = purrr::map(data, lag_polys)) %>%
  unnest(polys)

# minor break points
mb <- crossing(x = c(2.5, 5, 7.5), y = 10^(seq(-4, 5))) %>%
  mutate(z = x*y) %>%
  `[`("z") %>%
  unlist() %>%
  as.numeric

# text labels for polygon bands
band_label_pos <- panel_sel_breaks + 1 # this is a good default but may need customizing
band_labels <- tibble(
  y = band_label_pos,
  x = min(alphas),
  label = c("5-7 (26%-37%)\ninteresting panels", "")
)

# Finally, the actual plot
ggplot() +
  geom_polygon(aes(x = x, y = y, group = interaction(type, band)), data = bands, fill = "grey", alpha = 0.5) +
  geom_point(aes(x = alpha, y = n_interesting),data = prior_pred_mean, alpha = .05) +
  geom_line(aes(x = alpha, y = n_sel_plots), data = model_df, color = "blue", size = 1) +
  scale_x_log10(name = expression(alpha), breaks = c(0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000),
                minor_breaks = mb,
                labels = 10^(seq(-3, 4))) +
  geom_text(aes(x = x, y = y, label = label), data = band_labels, hjust = 0, color = "grey30", size = 3) +
  coord_cartesian(xlim = range(alphas)) +
  scale_y_continuous(name = paste0("Number of panels with at least ",
                                   interest_threshold, " selection"),
                     breaks = 1:100) +
  annotate("segment", x=0.000001, xend=selected_breakpoints$alpha,
               y=6, yend=6,
               colour="grey50") +
  annotate("segment", x=selected_breakpoints$alpha,
           xend=selected_breakpoints$alpha,
               y=-Inf, yend=6,
               colour="grey50")

@

Under the BB mixture model, the visual $p$-value for the data shown in the lineup in \Cref{fig:example-plot} is \Sexpr{round(new_vispval,2)} based on $\hat{\alpha} = 0.174$ corresponding to six interesting null plots. For a range of 5-7 interesting null plots, we have estimates for $\alpha$ between 0.1 and 0.25, corresponding to $p$-values between  $\Sexpr{round(new_vispval_high,2)}$ and  $\Sexpr{round(new_vispval_low,2)}$.
All of these values are still significant at the 0.05 level, however, these values are larger than the $p$-value computed under the Binomial model.
When we consider the actual results of the experimental evaluation of \Cref{fig:example-plot}, the $p$-value computed using the mixture model is much more plausible: the data plot is the most favored of all of the panels in the lineup, but it is not overwhelmingly significant; at least one other panel is almost as popular, and overall, null panels were still selected more frequently than the data panel.
This suggests that $p$-values generated using the BB mixture model are better calibrated than those generated by the Binomial model.


\section{Discussion}

In this paper, we have described three scenarios for visual inference experiments and developed a model for visual inference, that is effective even under Scenario 3, where dependencies complicate probability calculations. This model is a more general case of the model proposed in \citet{majumder2013validation}, but provides p-values that account for dependencies in successive evaluations of the same lineup by different participants.
The proposed Dirichlet-Multinomial (DM) model must be calibrated using a parameter $\alpha$ which describes the number of null panels in a lineup which are visually interesting. 
Using expected quantities that are easily observable, we describe two methods for estimation of $\alpha$: a visual method for use on lineups that contain data panels, and a numerical method for use on Rorschach lineups.
The $p$-values derived from the proposed DM model are more conservative than those produced by Majumder's Binomial model. Even though the latter is appropriate for Scenario 1, with the new model the use of $\hat\alpha$, estimated from the null plot generation method, provides an additional benefit: we calibrate the $p$-value to the lineups by accounting for the \emph{difficulty of the lineup evaluation under the null hypothesis}.

Visual inference leverages the strength of the perceptual system to test hypotheses, for a broad range of problems that can't be addressed with traditional tests. This comes with a cost: we cannot easily describe the null distribution or determine its parameters from the experimental design.
An exciting contribution of the visual and quantitative estimation methods proposed in this paper is that they allow us to determine the appropriate null distribution for a specific lineup experiment, with minimal cost to experimenters or participants.
By controlling for the perceptual difficulty of a task using quantitative parameters, we can produce visual $p$-values that account for the demands of the task  while still providing an assessment of the significance of the test statistic. 


Visual estimation of $\alpha$ also allows us to screen out lineup generation methods that are problematic: if a null plot generation method produces \dc{one} interesting plot out of a size $m=20$ Rorschach lineup, it is functionally impossible to distinguish a data plot which has a large signal from a null plot which also has a large signal. 
Extremely small $\alpha$ values serve as a signal that the null plot generation method is not visually appropriate because it sporadically (not consistently) generates visual features that are likely to catch the attention of participants (possibly for the wrong reasons).
We can guard against these anomalies by assessing $\alpha$ using a Rorschach pilot study, identifying any problems with the null plot generation method before the full study is conducted.

With methods for calculating $p$-values for visual inference studies conducted under Scenario 1 or Scenario 3, we can now approach the variety of study designs that might fall under Scenario 2 as described in this paper. 
Scenario 2 is an intermediate option between the two extremes of no dependence between successive lineup evaluations and complete dependence between successive lineup evaluations. 
As a result, by describing the calculation of $p$-values for Scenario 3, this paper lays a foundation for a comprehensive assessment of the visual $p$-values of different lineups under intermediate scenarios, too. 

\subsection*{Supporting Information} The code and data necessary to reproduce this article is available at \url{https://github.com/srvanderplas/visual-inference-alpha}.


% \nocite{*}% Show all bib entries - both cited and uncited; comment this line to view only cited bib entries;
\bibliography{references}%


% \section*{Author Biography}
% 
% \begin{biography}{
% \includegraphics[width=60pt,height=70pt,draft]{example-image-a}
% }{\textbf{Susan Vanderplas} is an Assistant Professor in the Department of Statistics, at the University of Nebraska-Lincoln. She received her PhD from the Department of Statistics at Iowa State University in 2015.}\end{biography}
% 
% \begin{biography}{\includegraphics[width=60pt,height=70pt,draft]{example-image-a}
% }{\textbf{Christian R\"ottger} is an Assistant Teaching Professor in the Department of Mathematics, at the Iowa State University. He received his PhD in Mathematics from the University of East Anglia in 2000.}\end{biography}
% 
% \begin{biography}{\includegraphics[width=60pt,height=70pt,draft]{example-image-a}
% }{\textbf{Dianne Cook} is Professor of Business Analytics in the Department of Econometrics and Business Statistics at Monash University. She received her PhD in Statistics from Rutgers University in 1993.}\end{biography}
% 
% \begin{biography}{\includegraphics[width=60pt,height=70pt,draft]{example-image-a}
% }{\textbf{Heike Hofmann} is a Professor in the Department of Statistics at Iowa State University. She received her PhD from the University of Augsburg in 2000.}
% \end{biography}

\appendix
\section{Visual \texorpdfstring{$p$}{p}-value distribution}\label{app:pvalue}
Assume, we have a lineup of size $m$ with $K$ evaluations resulting in $c_t$ target plot evaluations. We defined the BB model in \Cref{eqn:marginal-model-specification} leading to densities given as:

\begin{align*}
f(\theta \mid \alpha) & = \frac{1}{B\left(\alpha, (m-1)\alpha\right)}\cdot \theta^{\alpha - 1}(1-\theta)^{(m-1)\alpha - 1}\\[1em]
P(C = c_t  \mid K, \theta) & =  \binom{K}{c_t}\theta^{c_t}(1-\theta)^{K-c_t}
\end{align*}
% By Bayes Theorem, if $A_1 = C + \alpha$ and $A_2 = K - C + (m-1)\alpha$,
% \begin{align*}
% f(\theta|C, K, \alpha) &= \frac{f(\theta|\alpha) P(C | \theta)}{P(C = c)}\\
% &= \frac{B\left(\alpha, (m-1)\alpha\right)}{P(C=c)} \theta^{\alpha - 1}(1-\theta)^{(m-1)\alpha - 1}\cdot \binom{K}{C}\theta^C(1-\theta)^{K-C}\\
% &= \frac{1}{P(C=c)} B\left(\alpha, (m-1)\alpha\right) \binom{K}{C} \theta^{A_1 - 1}(1-\theta)^{A_2 - 1}\\
% \end{align*}
%
% As $f(\theta|C, K, \alpha)$ is a probability distribution, it integrates to 1. So we can infer that
% \begin{align*}
% P(C=c) &= \int B\left(\alpha, (m-1)\alpha\right) \binom{K}{C} \theta^{A_1 - 1}(1-\theta)^{A_2 - 1} d\theta\\
% & = B\left(\alpha, (m-1)\alpha\right) \binom{K}{C} \int \theta^{A_1 - 1}(1-\theta)^{X_2 - 1} d\theta\\
% & = B\left(\alpha, (m-1)\alpha\right) \binom{K}{C} \frac{B(A_1, A_2)}{B(A_1, A_2)} \int \theta^{A_1 - 1}(1-\theta)^{A_2 - 1} d\theta\\
% & =   \frac{B\left(\alpha, (m-1)\alpha\right) \binom{K}{C}}{B(A_1, A_2)\alpha)} \int B(A_1, A_2) \theta^{A_1 - 1}(1-\theta)^{A_2 - 1} d\theta\\
% & =  \frac{B\left(\alpha, (m-1)\alpha\right) \binom{K}{C} }{B\left(C+\alpha, K-C+(m-1)\alpha\right)} \\
% \end{align*}

We are interested in the probability of observing at least $c_t$ picks of the target plot assuming that the target plot is not inconsistent with the null plots generated from the null model, i.e. we are interested in the (unconditional) distribution of counts $C$. We get there by integrating over the rate parameter $\theta$.
From the theorem of total probability we know that 
\begin{eqnarray*}
P(C = c) &=& \int_0^1 P(C = c \mid \theta) f(\theta) d\theta
\end{eqnarray*}

Now we use that $C \mid \theta \sim$ Binom$_{\theta, K}$ and $\theta \sim$ Beta$_{\alpha, (m-1)\alpha}$:

\begin{eqnarray*}
P(C = c) &=&  \int_0^1 {\binom{K}{c}} \theta^c (1-\theta)^{K - c} \cdot
\frac{1}{B(\alpha, (m-1)\alpha)} \theta^{\alpha - 1}(1-\theta)^{(m-1)\alpha-1} d\theta = \\
&=& {\binom{K}{c}} \frac{1}{B(\alpha, (m-1)\alpha)} \underbrace{\int_0^1
 \theta^{c + \alpha - 1}(1-\theta)^{K-c + (m-1)\alpha-1} d\theta}_{\text{Beta function}} =\\
 &=& {\binom{K}{c}} \frac{B(c+\alpha, K-c + (m-1)\alpha)}{B(\alpha, (m-1)\alpha)}.
\end{eqnarray*}


Thus, the visual $p$-value for a lineup with $c_t$ target selections out of $K$ evaluations is
\begin{align}
P(C \geq c_t) & =  \frac{1}{B(\alpha, (m-1)\alpha) } \sum_{x=c_t}^K \binom{K}{x} B(x+\alpha, K-x+(m-1)\alpha).
\end{align}
A similar derivation holds in the full Dirichlet-Multinomial model.

\section{Expected number of panels picked}\label{app:expected}
Define $C = (C_1, ..., C_m) \sim \text{Mult}_{\theta, K}$ to be a (simulated) lineup that is evaluated $K$ times, and $\theta = (\theta_1, ..., \theta_m) \sim \text{Dir}_\alpha = (\alpha, ..., \alpha)$ with $\sum_i \theta_i = 1$.

With indicator function $I$, defined as 1 for true statements and 0 for false statements, we define:

\[
Z_c(\alpha) = \sum_{i=1}^{m} I(C_i \ge c),
\]
where $Z_c$ is the number of panels in a lineup that were picked at least $c$ times. We express this random variable as a function in $\alpha$ - the dependency becomes clear, once we look at the expected value of $Z_c$:

\[
E[Z_c(\alpha)] = \sum_{i=1}^m E\left[ I(C_i \ge c) \right] =
\sum_{i=1}^m P(C_i \ge c).
\]

The probabilities $P(C_i \ge c)$ are derived in the previous section as marginal Beta-binomials: 
\begin{equation}
E[Z_c(\alpha)] = \frac{m}{ B(\alpha, (m-1)\alpha)} \cdot \sum_{x=\lceil c \rceil}^K \binom{K}{x} B(x+\alpha, K-x+(m-1)\alpha).
\end{equation}

\section{Sub vectors of Dirichlet distributions}\label{neutrality}

Let $\bm\theta = (\theta_1, ... \theta_m)$ with $\bm\theta \sim $Dir$(\alpha)$ for some real-valued $\alpha > 0$. 
The Dirichlet distribution is neutral with respect to all possible partitions of the corresponding index set \citep{Connor:1969bt, doksum:74, Sakowicz:2014hi}.
This means that  $\theta_m$ is independent of the (normalized) random variable $\bm\theta^{-m} := (\frac{\theta_1}{1-\theta_m}, ..., \frac{\theta_{m-1}}{{1-\theta_m}})$. 
Further, this implies that the conditional distribution $\bm\theta^{-m} \mid \theta_m \sim $ Dir$(\alpha)$.

In the context of lineups this means that given the same null model generation we can assume for the probabilities to select a panel from a Rorschach lineup and the probabilities to select a panel corresponding to a null plot from a lineup Dirichlet distributions with the same parameter $\alpha > 0$. 
\end{document}
